<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>chatbot.YoloV5ObjectDetectionEngine API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>chatbot.YoloV5ObjectDetectionEngine</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import random
import sys
from pathlib import Path
from typing import List
from typing import Tuple
from typing import Dict

import cv2
import numpy
import numpy as np
import torch

sys.path.append(str(Path(__file__).parent.absolute()))
sys.path.append(str(Path(__file__).parent.absolute()) + &#34;/_yolov5&#34;)

from _yolov5.models.experimental import attempt_load
from _yolov5.utils.general import non_max_suppression

device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)
RED_COLOR = (0, 0, 255)

_detection_network = None
_classes = []
_class_colors = []


class ObjectDetection:

    def __init__(self,
                 model_path: str,
                 input_width: int = 320,
                 conf_threshold: float = 0.25,
                 iou_thres: float = 0.45) -&gt; None:
        &#34;&#34;&#34;Initialise the object detecion class.

        Args:
            model_path: Path to the yolov5 model on filesystem
            input_width: Input width to the model, default 640
            conf_threshold: Confidence threshold for non-maxima suppression
            iou_thres: IoU threshold for non-maxim suppression
        &#34;&#34;&#34;
        self.yolo_model = attempt_load(weights=model_path, map_location=device)
        self.input_width = input_width
        self.conf_threshold = conf_threshold
        self.iou_thres = iou_thres

    def detect(self, input_image: numpy.array) -&gt; List[Dict]:
        &#34;&#34;&#34;Run the input image trough YoloV5 Object detection neural network.

        Args:
            input_image: Input image as numpy array

        Returns: Inference results, list of dictionaries containing bounding boxes,labels and scores
        &#34;&#34;&#34;
        height, width = input_image.shape[:2]
        new_height = int((((self.input_width / width) * height) // 32) * 32)

        img = cv2.resize(input_image, (self.input_width, new_height))
        img = np.moveaxis(img, -1, 0)
        img = torch.from_numpy(img).to(device)
        img = img.float() / 255.0  # 0 - 255 to 0.0 - 1.0
        if img.ndimension() == 3:
            img = img.unsqueeze(0)

        pred = self.yolo_model(img, augment=False)[0]
        pred = non_max_suppression(pred,
                                   conf_thres=self.conf_threshold,
                                   iou_thres=self.iou_thres,
                                   classes=None)
        items = []

        if pred[0] is not None and len(pred):
            for p in pred[0]:
                score = np.round(p[4].cpu().detach().numpy(), 2)
                # label = self.classes[int(p[5])]
                label = int(p[5])
                xmin = int(p[0] * input_image.shape[1] / self.input_width)
                ymin = int(p[1] * input_image.shape[0] / new_height)
                xmax = int(p[2] * input_image.shape[1] / self.input_width)
                ymax = int(p[3] * input_image.shape[0] / new_height)

                item = {
                    &#39;label&#39;: label,
                    &#39;bbox&#39;: [(xmin, ymin), (xmax, ymax)],
                    &#39;score&#39;: score
                }

                items.append(item)

        return items


def _get_random_color() -&gt; Tuple:
    &#34;&#34;&#34;Get a random color for the cv2 plots.

    Returns: Tuple, with 3 values ranging from 0 to 255
    &#34;&#34;&#34;
    return random.randint(0, 255), random.randint(0,
                                                  255), random.randint(0, 255)


def load_network(model_path: str,
                 input_width: int = 640,
                 conf_threshold: float = 0.25,
                 iou_thres: float = 0.45,
                 classes: list = []) -&gt; None:
    &#34;&#34;&#34;Load the Yolov5 neural network for the inference.

    Classes parameter can be omitted. If not provided, drawing on the frame will use class numbers
    instead of class labels.

    Args:
        model_path: Path to the yolov5 model on filesystem
        input_width: Input width to the model, default 640
        conf_threshold: Confidence threshold for non-maxima suppression
        iou_thres: IoU threshold for non-maxim suppression
        classes: Array holding list of classes
    &#34;&#34;&#34;
    global _detection_network
    global _classes
    global _class_colors
    _detection_network = ObjectDetection(model_path, input_width,
                                         conf_threshold, iou_thres)
    _classes = classes
    if classes:
        for i in range(0, len(classes)):
            _class_colors.append(_get_random_color())
    else:
        for i in range(0, 1000):
            _class_colors.append(_get_random_color())


def _inference_frame(img: numpy.array) -&gt; List[Dict]:
    &#34;&#34;&#34;Runs the inference from the supplied cv2 frame.

    The detection network is must be loaded in via load_network() function before attempting inference

    Args:
        img: CV2 Type frame to run the inference on

    Returns: Inference results
    &#34;&#34;&#34;
    assert _detection_network is not None, &#34;You first need to load in your neural network via load_network()&#34;
    items = _detection_network.detect(img)
    return items


def _draw_on_frame(frame: numpy.array, items: List) -&gt; numpy.array:
    &#34;&#34;&#34;Draws provided inference results on the provided frame.

    Args:
        frame: Frame to draw on
        items: Inference results to draw

    Returns: Frame with inference results drawn on it
    &#34;&#34;&#34;
    for obj in items:
        label = obj[&#39;label&#39;]
        color = _class_colors[label]
        if _classes:
            label = _classes[label]
        score = obj[&#39;score&#39;]
        [(xmin, ymin), (xmax, ymax)] = obj[&#39;bbox&#39;]
        frame = cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)
        frame = cv2.putText(frame, f&#39;{label} ({str(score)})&#39;, (xmin, ymin),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.75, color, 1,
                            cv2.LINE_AA)
    return frame


def inference_from_file(img_path: str) -&gt; None:
    &#34;&#34;&#34;Runs the inference on frame after loading it in from the path provided.

    The inference results are displayed to the user via CV2 window

    Args:
        img_path: Path to the image on the filesystem
    &#34;&#34;&#34;
    img = cv2.imread(img_path)
    items = _inference_frame(img)
    img = _draw_on_frame(img, items)
    cv2.imshow(&#34;Inference results&#34;, img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()


def inference_on_camera(camera: str = &#34;/dev/video0&#34;) -&gt; None:
    &#34;&#34;&#34;Runs the inference from the frames incoming via camera provided.

    The inference results are displayed to the user via CV2 window


    Args:
        camera: System path to camera, /dev/video* on Linux
    &#34;&#34;&#34;
    camera = cv2.VideoCapture(camera)

    while True:
        ok, frame = camera.read()
        if not ok:
            raise IOError
        objs = _inference_frame(frame)
        frame = _draw_on_frame(frame, objs)
        cv2.imshow(&#34;Result&#34;, frame)
        key = cv2.waitKey(20)
        if key == ord(&#39;a&#39;):
            cv2.destroyAllWindows()
            break</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="chatbot.YoloV5ObjectDetectionEngine.inference_from_file"><code class="name flex">
<span>def <span class="ident">inference_from_file</span></span>(<span>img_path: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Runs the inference on frame after loading it in from the path provided.</p>
<p>The inference results are displayed to the user via CV2 window</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img_path</code></strong></dt>
<dd>Path to the image on the filesystem</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inference_from_file(img_path: str) -&gt; None:
    &#34;&#34;&#34;Runs the inference on frame after loading it in from the path provided.

    The inference results are displayed to the user via CV2 window

    Args:
        img_path: Path to the image on the filesystem
    &#34;&#34;&#34;
    img = cv2.imread(img_path)
    items = _inference_frame(img)
    img = _draw_on_frame(img, items)
    cv2.imshow(&#34;Inference results&#34;, img)
    cv2.waitKey(0)
    cv2.destroyAllWindows()</code></pre>
</details>
</dd>
<dt id="chatbot.YoloV5ObjectDetectionEngine.inference_on_camera"><code class="name flex">
<span>def <span class="ident">inference_on_camera</span></span>(<span>camera: str = '/dev/video0') ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Runs the inference from the frames incoming via camera provided.</p>
<p>The inference results are displayed to the user via CV2 window</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>camera</code></strong></dt>
<dd>System path to camera, /dev/video* on Linux</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inference_on_camera(camera: str = &#34;/dev/video0&#34;) -&gt; None:
    &#34;&#34;&#34;Runs the inference from the frames incoming via camera provided.

    The inference results are displayed to the user via CV2 window


    Args:
        camera: System path to camera, /dev/video* on Linux
    &#34;&#34;&#34;
    camera = cv2.VideoCapture(camera)

    while True:
        ok, frame = camera.read()
        if not ok:
            raise IOError
        objs = _inference_frame(frame)
        frame = _draw_on_frame(frame, objs)
        cv2.imshow(&#34;Result&#34;, frame)
        key = cv2.waitKey(20)
        if key == ord(&#39;a&#39;):
            cv2.destroyAllWindows()
            break</code></pre>
</details>
</dd>
<dt id="chatbot.YoloV5ObjectDetectionEngine.load_network"><code class="name flex">
<span>def <span class="ident">load_network</span></span>(<span>model_path: str, input_width: int = 640, conf_threshold: float = 0.25, iou_thres: float = 0.45, classes: list = []) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Load the Yolov5 neural network for the inference.</p>
<p>Classes parameter can be omitted. If not provided, drawing on the frame will use class numbers
instead of class labels.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_path</code></strong></dt>
<dd>Path to the yolov5 model on filesystem</dd>
<dt><strong><code>input_width</code></strong></dt>
<dd>Input width to the model, default 640</dd>
<dt><strong><code>conf_threshold</code></strong></dt>
<dd>Confidence threshold for non-maxima suppression</dd>
<dt><strong><code>iou_thres</code></strong></dt>
<dd>IoU threshold for non-maxim suppression</dd>
<dt><strong><code>classes</code></strong></dt>
<dd>Array holding list of classes</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_network(model_path: str,
                 input_width: int = 640,
                 conf_threshold: float = 0.25,
                 iou_thres: float = 0.45,
                 classes: list = []) -&gt; None:
    &#34;&#34;&#34;Load the Yolov5 neural network for the inference.

    Classes parameter can be omitted. If not provided, drawing on the frame will use class numbers
    instead of class labels.

    Args:
        model_path: Path to the yolov5 model on filesystem
        input_width: Input width to the model, default 640
        conf_threshold: Confidence threshold for non-maxima suppression
        iou_thres: IoU threshold for non-maxim suppression
        classes: Array holding list of classes
    &#34;&#34;&#34;
    global _detection_network
    global _classes
    global _class_colors
    _detection_network = ObjectDetection(model_path, input_width,
                                         conf_threshold, iou_thres)
    _classes = classes
    if classes:
        for i in range(0, len(classes)):
            _class_colors.append(_get_random_color())
    else:
        for i in range(0, 1000):
            _class_colors.append(_get_random_color())</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="chatbot.YoloV5ObjectDetectionEngine.ObjectDetection"><code class="flex name class">
<span>class <span class="ident">ObjectDetection</span></span>
<span>(</span><span>model_path: str, input_width: int = 320, conf_threshold: float = 0.25, iou_thres: float = 0.45)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialise the object detecion class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_path</code></strong></dt>
<dd>Path to the yolov5 model on filesystem</dd>
<dt><strong><code>input_width</code></strong></dt>
<dd>Input width to the model, default 640</dd>
<dt><strong><code>conf_threshold</code></strong></dt>
<dd>Confidence threshold for non-maxima suppression</dd>
<dt><strong><code>iou_thres</code></strong></dt>
<dd>IoU threshold for non-maxim suppression</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ObjectDetection:

    def __init__(self,
                 model_path: str,
                 input_width: int = 320,
                 conf_threshold: float = 0.25,
                 iou_thres: float = 0.45) -&gt; None:
        &#34;&#34;&#34;Initialise the object detecion class.

        Args:
            model_path: Path to the yolov5 model on filesystem
            input_width: Input width to the model, default 640
            conf_threshold: Confidence threshold for non-maxima suppression
            iou_thres: IoU threshold for non-maxim suppression
        &#34;&#34;&#34;
        self.yolo_model = attempt_load(weights=model_path, map_location=device)
        self.input_width = input_width
        self.conf_threshold = conf_threshold
        self.iou_thres = iou_thres

    def detect(self, input_image: numpy.array) -&gt; List[Dict]:
        &#34;&#34;&#34;Run the input image trough YoloV5 Object detection neural network.

        Args:
            input_image: Input image as numpy array

        Returns: Inference results, list of dictionaries containing bounding boxes,labels and scores
        &#34;&#34;&#34;
        height, width = input_image.shape[:2]
        new_height = int((((self.input_width / width) * height) // 32) * 32)

        img = cv2.resize(input_image, (self.input_width, new_height))
        img = np.moveaxis(img, -1, 0)
        img = torch.from_numpy(img).to(device)
        img = img.float() / 255.0  # 0 - 255 to 0.0 - 1.0
        if img.ndimension() == 3:
            img = img.unsqueeze(0)

        pred = self.yolo_model(img, augment=False)[0]
        pred = non_max_suppression(pred,
                                   conf_thres=self.conf_threshold,
                                   iou_thres=self.iou_thres,
                                   classes=None)
        items = []

        if pred[0] is not None and len(pred):
            for p in pred[0]:
                score = np.round(p[4].cpu().detach().numpy(), 2)
                # label = self.classes[int(p[5])]
                label = int(p[5])
                xmin = int(p[0] * input_image.shape[1] / self.input_width)
                ymin = int(p[1] * input_image.shape[0] / new_height)
                xmax = int(p[2] * input_image.shape[1] / self.input_width)
                ymax = int(p[3] * input_image.shape[0] / new_height)

                item = {
                    &#39;label&#39;: label,
                    &#39;bbox&#39;: [(xmin, ymin), (xmax, ymax)],
                    &#39;score&#39;: score
                }

                items.append(item)

        return items</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="chatbot.YoloV5ObjectDetectionEngine.ObjectDetection.detect"><code class="name flex">
<span>def <span class="ident">detect</span></span>(<span>self, input_image: <built-in function array>) ‑> List[Dict[~KT, ~VT]]</span>
</code></dt>
<dd>
<div class="desc"><p>Run the input image trough YoloV5 Object detection neural network.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_image</code></strong></dt>
<dd>Input image as numpy array</dd>
</dl>
<p>Returns: Inference results, list of dictionaries containing bounding boxes,labels and scores</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detect(self, input_image: numpy.array) -&gt; List[Dict]:
    &#34;&#34;&#34;Run the input image trough YoloV5 Object detection neural network.

    Args:
        input_image: Input image as numpy array

    Returns: Inference results, list of dictionaries containing bounding boxes,labels and scores
    &#34;&#34;&#34;
    height, width = input_image.shape[:2]
    new_height = int((((self.input_width / width) * height) // 32) * 32)

    img = cv2.resize(input_image, (self.input_width, new_height))
    img = np.moveaxis(img, -1, 0)
    img = torch.from_numpy(img).to(device)
    img = img.float() / 255.0  # 0 - 255 to 0.0 - 1.0
    if img.ndimension() == 3:
        img = img.unsqueeze(0)

    pred = self.yolo_model(img, augment=False)[0]
    pred = non_max_suppression(pred,
                               conf_thres=self.conf_threshold,
                               iou_thres=self.iou_thres,
                               classes=None)
    items = []

    if pred[0] is not None and len(pred):
        for p in pred[0]:
            score = np.round(p[4].cpu().detach().numpy(), 2)
            # label = self.classes[int(p[5])]
            label = int(p[5])
            xmin = int(p[0] * input_image.shape[1] / self.input_width)
            ymin = int(p[1] * input_image.shape[0] / new_height)
            xmax = int(p[2] * input_image.shape[1] / self.input_width)
            ymax = int(p[3] * input_image.shape[0] / new_height)

            item = {
                &#39;label&#39;: label,
                &#39;bbox&#39;: [(xmin, ymin), (xmax, ymax)],
                &#39;score&#39;: score
            }

            items.append(item)

    return items</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="chatbot" href="index.html">chatbot</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="chatbot.YoloV5ObjectDetectionEngine.inference_from_file" href="#chatbot.YoloV5ObjectDetectionEngine.inference_from_file">inference_from_file</a></code></li>
<li><code><a title="chatbot.YoloV5ObjectDetectionEngine.inference_on_camera" href="#chatbot.YoloV5ObjectDetectionEngine.inference_on_camera">inference_on_camera</a></code></li>
<li><code><a title="chatbot.YoloV5ObjectDetectionEngine.load_network" href="#chatbot.YoloV5ObjectDetectionEngine.load_network">load_network</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="chatbot.YoloV5ObjectDetectionEngine.ObjectDetection" href="#chatbot.YoloV5ObjectDetectionEngine.ObjectDetection">ObjectDetection</a></code></h4>
<ul class="">
<li><code><a title="chatbot.YoloV5ObjectDetectionEngine.ObjectDetection.detect" href="#chatbot.YoloV5ObjectDetectionEngine.ObjectDetection.detect">detect</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>